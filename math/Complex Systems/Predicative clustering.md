**Базовая концепция** — прогнозирование на основе кластеризации (predicative clustering). Идея заключается в том, что мы не строим прогнозирующую модель для всего ряда, как в классических или нейросетевых методах, а строим много мелких моделей, каждая из которых отвечает за определенный участок (различным областям странного аттрактора стоящего за рядом)  
Пытаемся найти также, как в [методе Розенштейна](Rosenstein%20algorithm.md) участки ряда, похожие друг на друга, сгруппировать (кластеризация) и найти центроиды кластеров. В теории они носят название **мотивов**. По факту это процедура *обучения*, а прогнозирование тогда, если мы завершили наблюдение в $t$ и прогнозируем в $t + 1, \dots, t + k$, то мы ищем среди обрезанных мотивов (первые $s - k$ точек) похожие и в качестве прогнозных значений используем последние $k$ точек  

Если ряд $y_1, \dots, y_t$ задается длиной мотива $s$, то мы обязаны рассмотреть все $z_i$ по этому ряду  
Над этим множеством осуществляем кластеризацию и получаем $C_{\alpha}$, которые разбивают $z_i$ на непересекающиеся множества. Центр каждого $C_{\alpha}$ будет центроид $\phi_{\alpha} = (\phi_1^{\alpha}, \dots, \phi_s^{\alpha})$. Тогда для ряда, наблюдение за которым мы завершили в момент времени $t$ мы рассматриваем весь спектр наблюдений $z_i^*=(y_{t - s + k}, \dots, y_t)$, а также множество обрезанных мотивов $\phi_{\alpha}^*=(\phi_1^{\alpha}, \dots, \phi_{s - k}^{\alpha})$, среди которых находим ближайший к вектору наблюдений  $\overline{\phi_{\alpha}} = arg\ min\ \rho(z^*, \phi_{\alpha}^*)$, тогда в качестве прогноза $\hat{y}_{t+j} = \overline{\phi_{s - k + j}^{\alpha}}$