## Плоскость энтропия-сложность

#### Информационная энтропия по Шэннону

Информация от итога события есть $I = f(\frac{1}{p})$, а от независимых событий ($p(a \cap b) = p(a)p(b)$) будет $I_a + I_b$  
В качестве функции $f$ подходит единственная возможная функция $ln \frac{1}{p} = -ln\ p$  
Пусть система находится в $i$-ом из $n$ состояний с вероятностью $p_i$  
Тогда количество информации, которое мы узнаем, увидев состояние системы и будет информационной энтропией  

$$
H = <I> = -\sum_{i = 1}^n p_i ln\ p_i
$$

Максимизируется, когда все события равновероятны, то есть $p_i = \frac{1}{n}$  
В плоскости энтропия сложность коэффициенты будут нормализованными, то есть $H_{norm} = \frac{H}{H_{max} = ln\ n}$, $0 \leq H_{norm} \leq 1$

$$
    c_i =
    \begin{cases}
        1, x_j < x_{j + 1} \\
        0, \text{иначе}
    \end{cases}
$$

$$
    p_i = p(c_i = 1)
$$

Тогда у детерминированного процесса (например $sin\ x$) $H \rightarrow 0$  
У абсолютно случайного процесса (например белый шум) $H \rightarrow 1$  
Хаотичные системы не помещаются в одну координату $\implies$ нам нужна ещё одна координата — сложность  

#### Сложность (MPR Complexity)

$$
    C_{MPR} = Q_0 \cdot D(P, P_e) \cdot H(p)
$$

где $P_e$ — равномерное распределение, $D(P_1, P_2)$ — мера несходства между двумя распределениями (в идеале дивергенция Кульбака-Лейблера, но нам подойдет и $L_2$ норма), $Q_0$ — нормирующий в $[0, 1]$ коэффициент  
На графике маленькая энтропия и сложность у детерминированных рядов, средняя энтропия и высокая сложность у хаотических рядов, средне-высокая энтропия и средняя сложность у реальных финансовых рядов, высокая энтропия и низкая сложность у случайных рядов (шумов)