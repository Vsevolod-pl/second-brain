## Метод Клозе-Шализи-Ньюмана

Простой процесс
- Оценить параметры $x_{min}$ и $\alpha$ модели степенного закона
- Подсчитать, насколько величина подходит под степенной закон. Если полученное значение $p$ не превышает $0.1$, то такую величину можно рассматривать, иначе эту гипотезу стоит отклонить.
- **(Необязательно)** Сравнить полученную модель с другими, что можно делать разными способами

Рассмотрим функцию распределения на интервале $[x_{min}, \infty]$, где $x_{min} > 0$

Отметим, что степенному закону может удовлетворять только “хвост” распределения, поэтому $x_{min}$ — не всегда минимальное значение случайной величины.

#### Покажем, как оценить $\alpha$

Для начала предположим, что $x_{min}$ уже определен (как его оценить будет написано ниже)

Самым хорошим методом в данном случае будет метод максимального правдоподобия (maximum likelihood) и для фиксированного $x_{min}$ и предполагаемого распределения строится оценочная функция максимального правдоподобия (maximum likelihood estimator, MLE), которой в непрерывном случае будет

$$
    \hat{\alpha} = 1 + n \left[ \sum_{i = 1}^n ln \frac{x_i}{x_{min}} \right]^{-1}
$$

Где $x_i$ — $i$-е из $n$ наблюдений случайной величины, при которых $x_i \geq x_{min}$.

Стандартное отклонение величины $\hat{\alpha}$ составляет

$$
    \sigma = \frac{\hat{\alpha} - 1}{\sqrt{n}} + O(\frac{1}{n})
$$

#### Приближение $x_{min}$ 

Для определения $x_{min}$ на эмпирических данных часто используют метод оценки глазом. Например, можно построить график с логарифмическими координатами и посмотреть, начиная с какого значения величина изменяется линейно, или пробовать оценивать $\alpha$ по различным $x_{min}$ и смотреть, начиная с какого значения получающаяся величина относительно стабильна. К сожалению, все эти методы склонны ошибаться из-за различных шумов и нестабильностей.

Рассмотрим аналитический способ нахождения $\hat{x}_{min}$, впервые предложенный Хэндкоком и Джонсом идея заключается в представлении всех данных одной моделью, включая данные меньше $x_min$. Для наблюдений с $x_i < x_{min}$ применяется модель на дискретных значениях, то есть $p_k = Pr(X = k)$. Можно заметить, что в таком случае приближение будет точнее и точнее с увеличением $x_{min}$, ведь оценочная функция будет иметь неограниченное число параметров и $MLE$ достигнет лучшего значения при $x_{min} \rightarrow \infty$. Стандартный (байесовский подход) в таких случаях — поиск максимального предельного правдоподобия (также называется обоснованностью), то есть правдоподобие данных с условием заданного числа параметров, проинтегрированное по возможным значениям параметра. Интеграл почти невозможно посчитать аналитически, но можно применить Лапласиан или метод наискорейшего спуска, в котором логарифмическое правдоподобие вынесено в старший (квадратичный) коэффициент и результирующий интеграл Гаусса может быть посчитан, как соответствующая матрица гессиан. Шварц показал, что в случае большого $n$ гессиан можно упростить до выражения

$$
    ln\ Pr(x|x_{min}) = \mathcal{L} - \frac{1}{2}x_{min}ln\ n
$$

Такое приближение называется критерием Байесовской информации (Bayesian information criterion, BIC), в свою очередь максимизация BIC и дает $\hat{x}_{min}$

#### Проверка полученных приближений

Есть множество способов проверить похожесть двух распределений, но самым простым/общеиспользуемым из них является тест Колмогорова-Смирнова. Очень легко определяется

$$
    D = max_{x \geq x_{min}} |S(x) - P(x)|
$$

Где $S(x)$ и $P(x)$ — функции распределения соответственных величин. Чем $D$ меньше, тем больше два распределения похожи.