Нет одного единого определения, что такое **хаотическая система**. Но есть признаки хаотических систем — **отпечатки пальцев хаоса**.  

**Горизонтом прогнозирования** называется определенное число шагов, на которое мы можем предсказывать состояние системы (пример: в задаче прогноза погоды с точностью до градуса горизонт прогнозирования составит 12 дней).  

Систему можно описать уравнением $\dot x = F(x)$, где значения — вектора.

**Операция линеаризации** — по нелинейной системе строим локально линейную.   Решать линейные системы много проще.  
Пусть $x_0(t)$ - решение системы, $x_0(t) + \varepsilon u(t)$ - возмущенная функция ($\varepsilon \ll 1$)  
Тогда при подстановке получаем

$$\dot x_0(t) + \varepsilon \dot u(t) = F(x_0(t) + \varepsilon u(t))$$

Что является функцией от параметра $\varepsilon$  
Далее можем разложить функцию по Тейлору в окрестности нуля

$$F(x_0(t) + \varepsilon u(t)) = F(x_0(t)) + \left[ \frac{\partial f_i}{\partial x_i} \bigg|_{x=x_0(t)} \right] \cdot \varepsilon u(t) + o(\varepsilon)$$

Откуда имеем, что

$$\cancel{\varepsilon} \dot u(t) \cong \left[ \frac{\partial f_i}{\partial x_i} \bigg|_{x=x_0(t)} \right] \cdot \cancel{\varepsilon} u(t) = A(t)u(t)$$

Что является линейным дифференциальным уравнением, где $A(t)$ — фиксированная матрица  
Найти $x_0(t)$ можно так

$$F(x) = 0 \rightarrow \widetilde{x_0} - \text{решение} \rightarrow x_0(t) = \widetilde{x_0}$$

Тогда

1) $A(t) \equiv A$
2) Теорема Гробмана-Хартмана — в окрестности особой точки $\widetilde{x_0}$ нелинейная система эквивалентна своей линеаризации

**Катастрофа** — резкий скачок системы при небольшом изменении параметров (в этом виноваты точки бифуркации системы — аттракторы и репеллеры)

Получили линеаризованную систему $\dot u(t) = A(t)u(t)$ и $x_0(t)$

___

**Показатель Ляпунова**  $$\lambda = \overline{\lim_{t\to\infty}} \frac{1}{t} ln || u(t) ||, \text{почти всегда можно отбросить верхний лимит}$$

Кажется, что определение показателя неверно, так как для различных $x_0(t)$ мы получим различные $u(t)$, но на самом деле этот показатель будет характеристикой всех хаотической системы.  

___

**Мультипликативная эргодическая теорема (Теорема Оселедца)** утверждает, что $\lambda$ может принимать ровно $n$ (размерность $x$) значений, которые в совокупности дают   **Ляпуновский спектр** — $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$,  которые можно найти по алгоритму Бенеттина, а наиболее важным является $\lambda_1$ — старший показатель Ляпунова. Он и является отпечатком хаоса в системе.

$$
\begin{cases}
\lambda_1 > 0, \text{в хаотических системах} \\
\lambda_1 < 0, \text{в регулярных системах} \\
\end{cases}
$$

Если же более одного значения спектра положительны, то система называется **гиперхаотической**, но обычно $\lambda_1 > 0, \lambda_2 = 0, \lambda_3 < 0, \ldots$  

___

Знак $\lambda_1$ связан с **устойчивостью траектории по Ляпунову**  
Траектория устойчива, если $\forall \varepsilon > 0 \ \exists \delta(\varepsilon)>0$ такое, что

$$||x(0) - \tilde{x}(0)|| < \delta \rightarrow ||x(t) - \tilde{x}(t)|| < \varepsilon$$

И аналогично неустойчива, если $\forall \varepsilon, \delta > 0, \exists T > 0$ такое, что $\forall t > T$

$$||x(t) - \tilde{x}(t)|| > \varepsilon \rightarrow \text{по I теореме Ляпунова} \rightarrow ||x(t) - \tilde{x}(t)|| \sim e^{\lambda_1 t}$$

Откуда имеет экспоненциальный рост и это не является траекторией в регулярной системе  
Соответственно все хаотические системы неустойчивы по Ляпунову

Если ошибка в момент времени $0$ равна $\varepsilon(0)$, то ошибка во время $t$ будет равна $\varepsilon(0) \cdot e^{\lambda t}$  
Если существует максимальная ошибка $\varepsilon_{max}$, то легко найти значение $T = max t$, среди всех значений удовлетворяющих максимальной ошибке  
В случае же $\lambda < 0$ получается, что $T = +\infty$

[Численное интегрирование](../Calculus/Numerical%20Integration.md)

**Алгоритм Бенеттина**
- Ортогонализируем вектора методом Грама-Шмидта и нормируем (показатель Ляпунова не меняется, значит, когда вектора сильно отклоняются от ортогональных, мы можем их ортонормировать)
- Применяем численное интегрирование к $\dot u = A(t)u$ $n$ раз с разными граничными условиями
- Получим $n$ решений $u_i$, они будут составлять спектр $\lambda(u_i)$
- Решает проблему того, что компонента $\lambda_1$ очень большая и численные методы склонили бы все вектора к ней, а мы хотим, чтобы вектора оставались ортогональными и не росли
___
В реальности у нас обычно нет $x_0(t)$ — решения исходной нелинейной системы
Запишем $n^2+n$ уравнений

$$
    \begin{cases}
        \dot x = F(x(t)) \\
        \dot u^{(i)} = A(t)u^{(i)} \\
        + \text{добавим условия} u^{(i)}(0) = V_i, \text{где все} V_i \text{ - ортогональны}
    \end{cases}
$$

Теперь можем численно интегрировать исходную систему и по ходу подставлять $x_0(t)$ в $A(t)$ и численно интегрировать это  
Просто интегрируем численно эту систему, периодически вмешиваясь и ортонормируя вектора. 
___
Кроме линейных показателей Ляпунова есть **объемные показатели Ляпунова**  
Пусть $u^{(1)}, u^{(2)}, \dots u^{(m)}$ — линейно независимые решения линеаризованной системы $\dot u = A(t)u$ .  
Если $m = n$, то это фундаментальная система $\rightarrow$ Вронскиан

$$
Vol(u^{(1)}, u^{(2)}, \dots u^{(m)}) = \sqrt{det \left( 
\begin{matrix}
(u^{(1)}(t), u^{(1)}(t)) & \dots & (u^{(1)}(t), u^{(m)}(t)) \\
\vdots & \ddots & \vdots \\
(u^{(m)}(t), u^{(1)}(t)) & \dots & (u^{(m)}(t), u^{(m)}(t)) 
\end{matrix}
\right)}
$$

Тогда объемный показатель Ляпунова равен

$$
\mathscr{H}_m = \lim_{t\rightarrow \infty} \frac{1}{t} ln |Vol(u^{(1)}, u^{(2)}, \dots u^{(m)})|
$$

Несколько свойств

$$
\begin{cases}
\mathscr{H}_m = \lambda_1 + \lambda_2 + \dots + \lambda_m \\
\lambda_i = \mathscr{H}_i - \mathscr{H}_{i - 1} \\
\mathscr{H}_n > 0 \rightarrow \text{система консервативна} \\
\mathscr{H}_n < 0 \rightarrow \text{система диссипативна (хаотична)} \\
\end{cases}
$$

___

**Вычисление показателей Ляпунова по временному ряду**

- Понятно, что редко уравнение системы задано явно, но часто у нас есть временной ряд изменения её наблюдаемых и измеряемых характеристик
- Восстановить систему $\dot x = f(x)$ хоть и возможно, но на 2 порядка сложнее оценки старшего показателя Ляпунова
- Для решения задачи можно использовать алгоритм Розенштейна
- Было доказано, что если траектории циклично похожи, то можем считать по теореме Ляпунова, что у нас есть возбужденная и невозбужденная функции

$$
    \varepsilon e^{\lambda l \Delta t} \cong \varepsilon(k \Delta t) \rightarrow \hat{\lambda} = \frac{1}{k \Delta t} ln \frac{\varepsilon(k \Delta t)}{\varepsilon(0)}
$$

1. Для каждого $z_i = (x_i, x_{i + 1}, \dots, x_{i + l - 1})$ находим множество его ближайших соседей $C = \{z_{i*} | \rho(z_i, z_{i*}) < \varepsilon\}$ и $|i - i*| > p$
2. Вместе с $z_i$ и его множеством ближайших соседей рассматриваем $z_{i + k}$ и $C$, благодаря чему находим оценку 

$$
\hat{\lambda_i} = \frac{1}{|C|} \sum_{z_{i*} \in C} \frac{1}{k \Delta t} ln \frac{\rho (z_{i*+k}, z_{i + k})}{\rho({z_{i*}, z_i})}
$$

3. Тогда хорошим приближением показателя будет

$$
    \hat{\lambda} = \frac{1}{|T|} \sum_{i = 1}^T \hat{\lambda_i}
$$

___

**Метод Розенштейна** — выбор по умолчанию при проверке ряда на хаотичность, много где реализован  
Есть модификации, которые повторяют основные идеи  
Наиболее популярен **метод аналога** — делим соседние траектории на фиксированное число шагов $k$, пока траектории не разойдутся на $\varepsilon_{max}$. В момент расхождения мы ищем нового ближайшего соседа из соображений гладкости возмущенной траектории

Недостатки
1. Зависит от вектора $|z|$, но это проблема снимается теоремой Такенса и алгоритмов построенных на ней
2. На реальных данных часто считает ряды хаотическими (дает маленькие положительные значение на рядах с шумом)

___

**Плоскость энтропия-сложность**

**Информационная энтропия по Шэннону** — информация от итога события есть $I = f(\frac{1}{p})$, а от независимых событий ($p(a \cap b) = p(a)p(b)$) будет $I_a + I_b$  
В качестве функции $f$ подходит единственная возможная функция $ln \frac{1}{p} = -ln\ p$  
Пусть система находится в $i$-ом из $n$ состояний с вероятностью $p_i$  
Тогда количество информации, которое мы узнаем, увидев состояние системы и будет информационной энтропией  

$$
H = <I> = -\sum_{i = 1}^n p_i ln\ p_i
$$

Максимизируется, когда все события равновероятны, то есть $p_i = \frac{1}{n}$  
В плоскости энтропия сложность коэффициенты будут нормализованными, то есть $H_{norm} = \frac{H}{H_{max} = ln\ n}$, $0 \leq H \leq 1$

$$
    c_i =
    \begin{cases}
        1, x_j < x_{j + 1} \\
        0, \text{иначе}
    \end{cases}
$$

$$
    p_i = p(c_i = 1)
$$

Тогда у детерминированного процесса (например $sin\ x$) $H \rightarrow 0$  
У абсолютно случайного процесса (например белый шум) $H \rightarrow 1$  
Хаотичные системы не помещаются в одну координату $\implies$ нам нужна ещё одна координата — сложность  

**Сложность (MPR Complexity)** равна

$$
    C_{MPR} = Q_0 \cdot D(P, P_e) \cdot H(p)
$$

где $P_e$ — равномерное распределение, $D(P_1, P_2)$ — мера несходства между двумя распределениями (в идеале дивергенция Кульбака-Лейблера, но нам подойдет и $L_2$ норма), $Q_0$ — нормирующий в $[0, 1]$ коэффициент  
На графике маленькая энтропия и сложность у детерминированных рядов, средняя энтропия и высокая сложность у хаотических рядов, средне-высокая энтропия и средняя сложность у реальных финансовых рядов, высокая энтропия и низкая сложность у случайных рядов (шумов)

___

**Прогнозирование за горизонтом прогнозирования**

Горизонт прогнозирования — физическое свойство системы и прогнозировать за ним нельзя. Существующие методы позволяют уйти за 20-30% горизонта, а далее, в силу Ляпуновской неустойчивости, начинается катастрофическая потеря точности.  
**Базовая концепция** — прогнозирование на основе кластеризации (predicative clustering). Идея заключается в том, что мы не строим прогнозирующую модель для всего ряда, как в классических или нейросетевых методах, а строим много мелких моделей, каждая из которых отвечает за определенный участок (различным областям странного аттрактора стоящего за рядом)  
Пытаемся найти также, как в методе Розенштейна участки ряда, похожие друг на друга, сгруппировать (кластеризация) и найти центроиды кластеров. В теории они носят название **мотивов**. По факту это процедура *обучения*, а прогнозирование тогда 

#TODO 

___

**Теорема Такенса (основная теорема прогнозирования)**

В общем случае у нас есть просто временной ряд. Есть переходные процессы и установившиеся состояния, в первом случае состояние движется к аттрактору, во втором случае изменяется вблизи аттрактора. Можно рассматривать геометрическое тело отдельно от пространства и пытаться вложить его в разные пространства, но вкладывать в пространства меньшей размерности плохо. В уравнении, размерности которого мы не знаем и наблюдаем только 1 параметр в ряду (или несколько, но мало) выбираем параметр $l$ — размер векторов $z$ (несколько наблюдений подряд) и получаем вложение в пространство размерности $l$.  
Параметр $l$ можно получить из **теоремы Уитни** — $m \geq 2d + 1$. Если размерность наблюдаемой функции $d$, то гарантированно можно вложить в пространство размера $m$ и больше.  
Диффеоморфизм между $M$ и $N$ — $g: M \rightarrow N$
1. Взаимооднозначное соответствие
2. $g$ и $g^{-1}$ дифференцируемы нужное число раз

Теорема Такенса  
Рассмотрим взаимооднозначное отображение из многообразия размерности $d$ в $R^m$ — $M^d \rightarrow R^m$, которое как минимум дважды дифференцируемо и $m = 2d + 1 = n$, все неподвижные точки и циклы длины $k$ различаются между собой (по значениям). Тогда $g$ — диффеоморфизм, который вкладывает $M^d$ в $R^m$. (На практике условием про равенство обычно пренебрегают)  
Тот геометрический объект, вокруг которого происходит вращение истинной хаотической системы не многообразен даже локально (сложный фрактальный объект), однако может быть помещен в минимальное инерционное многообразие и можно считать, что динамика происходит на нем $\implies$ применяется теорема Такенса.

Следствия из теоремы Такенса  
1. Пусть $x_i \in M^d$ и $g: M^d \rightarrow R^n, g(x_i) = y_i$. Рассмотрим малое время $\tau$, и рассмотрим $x_i$ через время $\tau$. Эта точка отобразится в $y_{i + 1} = g(x_{i + 1})$
2. Рассмотрим отображение связывающее $y_i$ и $y_{i + 1} \dots$
3. #TODO 

        